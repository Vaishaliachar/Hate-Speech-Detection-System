{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Detection System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.initializers import glorot_uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(list(string.punctuation))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>cyberbullying_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In other words #katandandre, your food was cra...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why is #aussietv so white? #MKR #theblock #ImA...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>@XochitlSuckkks a classy whore? Or more red ve...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@Jason_Gio meh. :P  thanks for the heads up, b...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>@RudhoeEnglish This is an ISIS account pretend...</td>\n",
       "      <td>not_cyberbullying</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          tweet_text cyberbullying_type\n",
       "0  In other words #katandandre, your food was cra...  not_cyberbullying\n",
       "1  Why is #aussietv so white? #MKR #theblock #ImA...  not_cyberbullying\n",
       "2  @XochitlSuckkks a classy whore? Or more red ve...  not_cyberbullying\n",
       "3  @Jason_Gio meh. :P  thanks for the heads up, b...  not_cyberbullying\n",
       "4  @RudhoeEnglish This is an ISIS account pretend...  not_cyberbullying"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"cyberbullying_tweets.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = df['tweet_text']\n",
    "y = df['cyberbullying_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        In other words #katandandre, your food was cra...\n",
       "1        Why is #aussietv so white? #MKR #theblock #ImA...\n",
       "2        @XochitlSuckkks a classy whore? Or more red ve...\n",
       "3        @Jason_Gio meh. :P  thanks for the heads up, b...\n",
       "4        @RudhoeEnglish This is an ISIS account pretend...\n",
       "                               ...                        \n",
       "47687    Black ppl aren't expected to do anything, depe...\n",
       "47688    Turner did not withhold his disappointment. Tu...\n",
       "47689    I swear to God. This dumb nigger bitch. I have...\n",
       "47690    Yea fuck you RT @therealexel: IF YOURE A NIGGE...\n",
       "47691    Bro. U gotta chill RT @CHILLShrammy: Dog FUCK ...\n",
       "Name: tweet_text, Length: 47692, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_simple_pos(tag) :\n",
    "    if tag.startswith('J') :\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V') :\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N') :\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R') :\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def clean_text(review) :\n",
    "    global max_len\n",
    "    words = word_tokenize(review)\n",
    "    output_words = []\n",
    "    for word in words :\n",
    "        if word.lower() not in stop_words :\n",
    "            pos = pos_tag([word])\n",
    "            clean_word = lemmatizer.lemmatize(word,pos = get_simple_pos(pos[0][1]))\n",
    "            output_words.append(clean_word.lower())\n",
    "    max_len = max(max_len, len(output_words))\n",
    "    return \" \".join(output_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\vaish\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In other words #katandandre, your food was crapilicious! #mkr\n",
      "word katandandre food crapilicious mkr\n"
     ]
    }
   ],
   "source": [
    "print(messages[0])\n",
    "messages = [clean_text(message) for message in messages]\n",
    "print(messages[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r', encoding=\"utf8\") as file:\n",
    "        word_to_vec_map = {}\n",
    "        word_to_index = {}\n",
    "        index_to_word = {}\n",
    "        index = 0\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            word_to_index[curr_word] = index\n",
    "            index_to_word[index] = curr_word\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "            index += 1\n",
    "    return word_to_index, index_to_word, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    m = len(X)\n",
    "    X_indices = np.zeros((m, max_len))\n",
    "    for i in range(m):\n",
    "        sentence_words = [w.lower() for w in X[i].split()]\n",
    "        j = 0\n",
    "        for word in sentence_words:\n",
    "            if word in word_to_index:\n",
    "                X_indices[i, j] = word_to_index[word]\n",
    "            j += 1\n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    vocab_len = len(word_to_index) + 1\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]\n",
    "    \n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable=False)\n",
    "    \n",
    "    embedding_layer.build((None,))\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NLPModel(input_shape, word_to_vec_map, word_to_index):\n",
    "    sentence_indices = Input(input_shape, dtype='int32')\n",
    "    \n",
    "    embeddings = pretrained_embedding_layer(word_to_vec_map, word_to_index)(sentence_indices)\n",
    "    \n",
    "    X = LSTM(128, return_sequences=True)(embeddings)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    X = Dropout(0.5)(X)\n",
    "    X = Dense(1)(X)\n",
    "    X = Activation('sigmoid')(X)\n",
    "    model = Model(inputs=sentence_indices, outputs=X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 505)]             0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 505, 50)           20000050  \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 505, 128)          91648     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 505, 128)          0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 128)               131584    \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 129       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20,223,411\n",
      "Trainable params: 223,361\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = NLPModel((max_len,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(messages, y, random_state = 0, test_size = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.7026 - accuracy: 0.5100\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6947 - accuracy: 0.5125\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6928 - accuracy: 0.5013\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6909 - accuracy: 0.5250\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6892 - accuracy: 0.5412\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 0s 963us/step - loss: 0.6879 - accuracy: 0.5512\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6873 - accuracy: 0.5462\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6858 - accuracy: 0.5475\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6852 - accuracy: 0.5663\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6842 - accuracy: 0.5700\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 0s 981us/step - loss: 0.6837 - accuracy: 0.5587\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 0s 975us/step - loss: 0.6824 - accuracy: 0.5875\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.6823 - accuracy: 0.5900\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 0s 997us/step - loss: 0.6810 - accuracy: 0.5663\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6819 - accuracy: 0.5913\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 0s 969us/step - loss: 0.6804 - accuracy: 0.5562\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6782 - accuracy: 0.5913\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 0s 994us/step - loss: 0.6779 - accuracy: 0.6050\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6779 - accuracy: 0.5763\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6780 - accuracy: 0.5863\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6772 - accuracy: 0.5962\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6764 - accuracy: 0.5813\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6748 - accuracy: 0.6012\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6727 - accuracy: 0.5863\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6724 - accuracy: 0.6087\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6722 - accuracy: 0.5888\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6699 - accuracy: 0.6162\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 0s 2ms/step - loss: 0.6686 - accuracy: 0.6025\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6687 - accuracy: 0.6150\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6670 - accuracy: 0.6087\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6653 - accuracy: 0.6212\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6643 - accuracy: 0.6150\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6635 - accuracy: 0.6062\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6630 - accuracy: 0.6175\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6614 - accuracy: 0.6263\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6599 - accuracy: 0.6100\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6582 - accuracy: 0.6300\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6569 - accuracy: 0.6400\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6552 - accuracy: 0.6400\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6542 - accuracy: 0.6313\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6521 - accuracy: 0.6325\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6506 - accuracy: 0.6388\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6512 - accuracy: 0.6375\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6481 - accuracy: 0.6388\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6472 - accuracy: 0.6450\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6456 - accuracy: 0.6425\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6445 - accuracy: 0.6237\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6420 - accuracy: 0.6438\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6416 - accuracy: 0.6538\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 0s 1ms/step - loss: 0.6396 - accuracy: 0.6463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2726af40518>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "# Example data generation (replace this with your actual data)\n",
    "# Here we're generating some synthetic data for demonstration.\n",
    "num_samples = 1000\n",
    "X = np.random.rand(num_samples, 10)  # Features\n",
    "Y = np.random.choice(['0', '1'], num_samples)  # Binary labels as strings\n",
    "\n",
    "# Convert Y to numeric\n",
    "le = LabelEncoder()\n",
    "Y = le.fit_transform(Y)  # Convert string labels to integers (0 and 1)\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Dense(32, input_dim=X_train.shape[1], activation='relu'))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))  # Output layer for binary classification\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, Y_train, epochs=50, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 1ms/step - loss: 0.7015 - accuracy: 0.5200\n",
      "Test Loss: 0.7015237212181091, Test Accuracy: 0.5199999809265137\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, Y_test)\n",
    "print(f\"Test Loss: {loss}, Test Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of LSTM: 67.25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['suck']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"suck it\"\n",
    "text = [clean_text(text)]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = sentences_to_indices(text, word_to_index, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[23695.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "            0.]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Define and fit the tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000)  # Adjust num_words as needed\n",
    "tokenizer.fit_on_texts(messages)\n",
    "# Tokenization (this is an example, use your actual tokenizer)\n",
    "# Assuming tokenizer was created and fit on your training data\n",
    "sequences = tokenizer.texts_to_sequences([messages])  # Convert text to sequences\n",
    "padded_sequences = pad_sequences(sequences, maxlen=10)  # Adjust maxlen to your training input size\n",
    "\n",
    "# Make the prediction\n",
    "prediction = model.predict(padded_sequences)\n",
    "print(prediction[0][0])  # Adjust indexing based on your output layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(word_to_index, open('word_to_index.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
